{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVNs70x16L6h",
        "outputId": "58d7b6c2-8e77-4a8f-8d5b-b7b83a65f518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#!wget -q https://mirrors.estointernet.in/apache/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz -P /content/drive/MyDrive # link wrong in blog\n",
        "!tar xf /content/drive/Shareddrives/DA231-2021-Aug-Public/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "45XOmcwm6OSd"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"Colab\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "spark\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import sys\n",
        "import json\n",
        "#from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.types import *\n",
        "#from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_rZgKQhc5Oax"
      },
      "outputs": [],
      "source": [
        "# list of 182 users\n",
        "num_enabled_users = 10\n",
        "user_list = list(range(num_enabled_users))\n",
        "if (128 in user_list):\n",
        "  user_list.remove(128)\n",
        "# define schema\n",
        "schema = StructType([\n",
        "    StructField(\"Lat\", StringType(), True), StructField(\"Long\", StringType(), True), StructField(\"Zero\", StringType(), True), StructField(\"Alt\", StringType(), True), StructField(\"DayCount\", StringType(), True), StructField(\"Date\", StringType(), True), StructField(\"Time\", StringType(), True), StructField(\"UserId\", StringType(), True)\n",
        "])\n",
        "\n",
        "# empty combined df\n",
        "df_combine = spark.createDataFrame([], schema)\n",
        "\n",
        "\n",
        "for ii in range(len(user_list)):\n",
        "  folder_name = \"{0:0=3d}\".format(user_list[ii])\n",
        "  data_user = \"/content/drive/MyDrive/DataEngAtScaleProject/GeolifeTrajectories/Data/\"+folder_name+\"/Trajectory/*.plt\"\n",
        "  #data_user = \"/content/drive/MyDrive/DataEngAtScaleProject/GeolifeTrajectories/Data/000/Trajectory/20081023025304.plt\"\n",
        "  #data000 = \"/content/drive/MyDrive/DataEngAtScaleProject/GeolifeTrajectories/Data/000/Trajectory/20081023025304.plt\"\n",
        "\n",
        "  # read as rdd\n",
        "  read_user = sc.textFile(data_user)\n",
        "\n",
        "  # remove header lines and split the values\n",
        "  rdd_user = read_user.filter(lambda x: len(x)>40).map(lambda l :  l.split(\",\"))\n",
        "\n",
        "  # convert rdd to dataframe\n",
        "  df_user = spark.createDataFrame(rdd_user).toDF(\"Lat\",\"Long\",\"Zero\",\"Alt\",\"DayCount\",\"Date\",\"Time\").withColumn(\"UserId\",F.lit(folder_name))\n",
        "  df_combine = df_combine.union(df_user)\n",
        "\n",
        "df_combine = df_combine.select(F.col(\"Lat\").cast(DoubleType()).alias(\"Lat\"), F.col(\"Long\").cast(DoubleType()).alias(\"Long\"), F.col(\"Alt\").cast(IntegerType()).alias(\"Alt\"), F.col(\"DayCount\").cast(DoubleType()).alias(\"DayCount\"), F.col(\"Date\").cast(DateType()).alias(\"Date\"), \"Time\", \"UserId\").cache()\n",
        "\n",
        "#print(df_combine.take(5))\n",
        "#print(df_combine.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Rpnz8W86Yn-n"
      },
      "outputs": [],
      "source": [
        "# UDF\n",
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
        "\n",
        "def GetCityName(Coord):\n",
        "  location = geolocator.reverse(Coord, language='en')\n",
        "  address = location.raw['address']\n",
        "  city = address.get('city', '')\n",
        "  return city\n",
        "\n",
        "GetCityName_UDF = F.udf(GetCityName, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Il1NPZDwr9j"
      },
      "outputs": [],
      "source": [
        "AggregateByDay = df_combine.withColumn(\"Coord\",F.concat_ws(\",\",(\"Lat\"),(\"Long\"))).select(\"Coord\",\"Date\",\"UserId\").groupBy(\"Date\",\"UserId\").agg(F.first(\"Coord\").alias(\"Coord\")).cache()\n",
        "CityUserDate = AggregateByDay.withColumn(\"CityName\",GetCityName_UDF(F.col(\"Coord\"))).select(\"CityName\",\"UserId\",\"Date\")\n",
        "CityList = CityUserDate.select(\"CityName\").distinct().collect()\n",
        "CityListFile = open(\"/content/drive/MyDrive/DataEngAtScaleProject/CityListOutputLog.txt\", \"w\")\n",
        "for row in CityList:\n",
        "  CityListFile.writelines(row[\"CityName\"] + \",\")\n",
        "\n",
        "CityListFile.writelines('\\n')\n",
        "CityUser = CityUserDate.groupBy(\"UserId\",\"CityName\").count().groupBy(\"UserId\").agg(F.max(F.struct(F.col(\"count\"),F.col(\"CityName\"))).alias(\"max\")).select(F.col(\"UserId\"), F.col(\"max.CityName\")).cache()\n",
        "CityUserSorted = CityUser.withColumn(\"UserNo\",CityUser[\"UserId\"].cast(IntegerType())).drop(\"UserId\").sort(\"UserNo\").collect()\n",
        "import csv\n",
        "with open('/content/drive/MyDrive/DataEngAtScaleProject/CityAllUserSorted.csv', 'w') as f:\n",
        "    for row in CityUserSorted:\n",
        "        f.write(\"%d,%s,%s\\n\"%(row[\"UserNo\"],\",\",row[\"CityName\"]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Most_Recorded_common_days = AggregateByDay.select(\"Date\").groupBy(\"Date\").count()\n",
        "Most_Recorded_common_days.toPandas().to_csv(\"/content/drive/MyDrive/DataEngAtScaleProject/CommonRecordedDays.txt\")\n",
        "CommonRecordedMonths = Most_Recorded_common_days.withColumn(\"YearMonth\",F.concat_ws(\"-\",F.split(\"Date\",\"-\").getItem(0),F.split(\"Date\",\"-\").getItem(1))).select(\"YearMonth\",\"count\").groupBy(\"YearMonth\").agg(F.sum(\"count\").alias(\"NumUserDays\")).sort(\"NumUserDays\",ascending = False)\n",
        "#CommonRecordedMonths.toPandas().to_csv(\"/content/drive/MyDrive/DataEngAtScaleProject/CommonRecordedMonths.txt\")"
      ],
      "metadata": {
        "id": "AAXz-R52XxSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZiiVXXtCzDl"
      },
      "outputs": [],
      "source": [
        "AggregateByMinutes = df_combine.where(F.col(\"Date\")>=\"2008-05-01\").where(F.col(\"Date\")<=\"2009-07-31\").select(\"Lat\",\"Long\",\"Date\",\"UserId\",F.concat_ws(\":\",F.split(\"Time\",\":\").getItem(0),F.split(F.split(\"Time\",\":\").getItem(1),'').getItem(0)).alias(\"Minutes\")).groupBy(\"Date\",\"Minutes\",\"UserId\").agg(F.first(\"Lat\").alias(\"Lat\"),F.first(\"Long\").alias(\"Long\")).cache()\n",
        "LessPreciseCoord10MinInterval = AggregateByMinutes.withColumn(\"ShortLat\",AggregateByMinutes.Lat.cast(DecimalType(scale=5))).withColumn(\"ShortLong\",AggregateByMinutes.Long.cast(DecimalType(scale=5))).drop(\"Lat\").drop(\"Long\").cache()\n",
        "#LessPreciseCoord10MinInterval.toPandas().to_csv(\"/content/drive/MyDrive/DataEngAtScaleProject/LessPreciseCoord10MinInterval.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LessPreciseCoord10MinInterval = spark.read.option(\"header\",True).csv(\"/content/drive/MyDrive/DataEngAtScaleProject/LessPreciseCoord10MinInterval.txt\")\n",
        "LessPreciseCoord10MinInterval = LessPreciseCoord10MinInterval.select(\"UserId\",F.col(\"ShortLat\").cast(DoubleType()).alias(\"ShortLat\"), F.col(\"ShortLong\").cast(DoubleType()).alias(\"ShortLong\"),F.col(\"Date\").cast(DateType()).alias(\"Date\"),\"Minutes\").cache()\n",
        "LessPreciseCoord10MinInterval.printSchema()"
      ],
      "metadata": {
        "id": "kWk1IiKoKFyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59B5Jeq0Ivfi"
      },
      "outputs": [],
      "source": [
        "\n",
        "UserListForSocRel = LessPreciseCoord10MinInterval.select(\"UserId\").distinct().sort(\"UserId\").collect()\n",
        "Per10MinData_user = {}\n",
        "for ii in range(len(UserListForSocRel)):\n",
        "  user_name = UserListForSocRel[ii][0]\n",
        "  Per10MinData_user[user_name] = LessPreciseCoord10MinInterval.where(F.col(\"UserId\")==user_name).cache()\n",
        "\n",
        "separationPerIter = open(\"/content/drive/MyDrive/DataEngAtScaleProject/perIterSeparationLog.txt\", \"a\")\n",
        "separationPerIter.writelines(\"New Run Starts Here\")\n",
        "separationPerIter.writelines('\\n')\n",
        "\n",
        "for ii in range(len(UserListForSocRel)):\n",
        "  SeparationTable = {}\n",
        "  CountTable = {}\n",
        "  for jj in range(ii+1,len(UserListForSocRel)):\n",
        "    user_name1 = UserListForSocRel[ii][0]\n",
        "    user_name2 = UserListForSocRel[jj][0]\n",
        "    #if CityUserSorted[ii][0] == CityUserSorted[jj][0]:\n",
        "    df1 = Per10MinData_user[user_name1].withColumnRenamed(\"ShortLat\",\"Lat1\").withColumnRenamed(\"ShortLong\",\"Long1\")\n",
        "    df2 = Per10MinData_user[user_name2].withColumnRenamed(\"ShortLat\",\"Lat2\").withColumnRenamed(\"ShortLong\",\"Long2\")\n",
        "    joinDF = df1.join(df2,[\"Date\",\"Minutes\"]).cache()\n",
        "    commonTimeCount = joinDF.count()\n",
        "    if commonTimeCount>50:   #atleast consider 50 common timestamps\n",
        "      tempVar = joinDF.withColumn(\"AbsDiff\",F.abs(F.col(\"Lat1\")-F.col(\"Lat2\"))+F.abs(F.col(\"Long1\")-F.col(\"Long2\"))).select(\"AbsDiff\").agg(F.avg(\"AbsDiff\").alias(\"MeanSeparation\")).collect()[0][0]\n",
        "      SeparationTable[(user_name1,user_name2)] = float(tempVar)\n",
        "      CountTable[(user_name1,user_name2)] = commonTimeCount\n",
        "  if bool(SeparationTable):\n",
        "    ClosestUsers = min(SeparationTable, key=SeparationTable.get)\n",
        "    separationPerIter.writelines(str(ClosestUsers))\n",
        "    separationPerIter.writelines(\":\")\n",
        "    separationPerIter.writelines(str(min(SeparationTable.values())))\n",
        "    separationPerIter.writelines(\":\")\n",
        "    separationPerIter.writelines(str(CountTable[ClosestUsers]))    \n",
        "    separationPerIter.writelines(\"\\n\")\n",
        "\n",
        "separationPerIter.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Connected_Users.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}